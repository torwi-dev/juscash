"""üï∑Ô∏è Scraper DJE S√£o Paulo"""

import asyncio
import time
import re
import io
from datetime import datetime, date, timedelta
from typing import List, Optional, Dict, Any
from urllib.parse import urljoin, parse_qs, urlparse

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import (
   TimeoutException, 
   NoSuchElementException, 
   WebDriverException,
   StaleElementReferenceException
)
from webdriver_manager.chrome import ChromeDriverManager

from bs4 import BeautifulSoup
import structlog
import httpx
import pdfplumber
import pytesseract
from PIL import Image
from decimal import Decimal

import html
from urllib.parse import urlparse, urlunparse

from ..config.settings import settings
from ..models.publication import PublicationData, ScrapingResult
from ..utils.circuit_breaker import CircuitBreaker


logger = structlog.get_logger(__name__)

class DJEScraperError(Exception):
   """üö® Erro do scraper DJE"""
   pass

class DJEScraper:
   """üï∑Ô∏è Scraper do Di√°rio da Justi√ßa Eletr√¥nico - COM DEBUG MELHORADO"""
   
   def __init__(self):
       self.driver: Optional[webdriver.Chrome] = None
       self.wait: Optional[WebDriverWait] = None
       self.current_execution_id: Optional[int] = None
       self.base_url = settings.dje_base_url
       self.search_url = settings.dje_search_url
       
       # HTTP client para downloads
       self.http_client = httpx.AsyncClient(
           timeout=httpx.Timeout(settings.pdf_timeout),
           headers={
               'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
           }
       )
       
       # Circuit breaker para prote√ß√£o
       self.circuit_breaker = CircuitBreaker(
           failure_threshold=3,
           recovery_timeout=30,
           expected_exception=WebDriverException
       )
       
       logger.info("DJE Scraper initialized (PDF STRATEGY - DEBUG MODE)")
   
   async def setup_driver(self):
       """üöó Configurar driver do Selenium usando Selenium Manager nativo"""
       try:
           options = settings.get_browser_options()
       
           logger.info("Starting Chrome browser with Selenium Manager", headless=settings.headless_browser)
       
           # Selenium Manager automaticamente baixa e configura o ChromeDriver
           self.driver = webdriver.Chrome(options=options)
           self.driver.implicitly_wait(settings.implicit_wait)
           self.driver.set_page_load_timeout(settings.browser_timeout)
           self.wait = WebDriverWait(self.driver, settings.browser_timeout)
       
           logger.info("Chrome driver setup completed successfully")
       
       except Exception as e:
           logger.error("Failed to setup Chrome driver", error=str(e))
           raise DJEScraperError(f"Driver setup failed: {str(e)}")
   
   async def navigate_to_search_page(self) -> bool:
       """üåê Navegar para p√°gina de busca avan√ßada"""
       try:
           with self.circuit_breaker:
               logger.info("Navigating to DJE advanced search", url=self.search_url)
               
               self.driver.get(self.search_url)
               
               # Wait for form to load
               self.wait.until(
                   EC.presence_of_element_located((By.NAME, "consultaAvancadaForm"))
               )
               
               logger.info("Successfully navigated to DJE search page")
               return True
               
       except Exception as e:
           logger.error("Failed to navigate to search page", error=str(e))
           return False
   
   async def configure_search_parameters(self, target_date: date) -> bool:
       """‚öôÔ∏è Configurar par√¢metros de busca CORRIGIDOS"""
       try:
           logger.info(
               "Configuring search parameters",
               target_date=target_date.isoformat(),
               search_terms=settings.search_terms,
               target_caderno=settings.target_caderno
           )
           
           # 1. Configurar datas
           date_str = target_date.strftime("%d/%m/%Y")
           
           # Data in√≠cio
           dt_inicio = self.wait.until(
               EC.presence_of_element_located((By.NAME, "dadosConsulta.dtInicio"))
           )
           self.driver.execute_script("arguments[0].removeAttribute('readonly')", dt_inicio)
           dt_inicio.clear()
           dt_inicio.send_keys(date_str)
           
           # Data fim (mesmo dia)
           dt_fim = self.driver.find_element(By.NAME, "dadosConsulta.dtFim")
           self.driver.execute_script("arguments[0].removeAttribute('readonly')", dt_fim)
           dt_fim.clear()
           dt_fim.send_keys(date_str)
           
           await asyncio.sleep(1)
           
           # 2. Selecionar Caderno CORRETO (value="12" = Caderno 3 - Parte I)
           caderno_select = Select(
               self.driver.find_element(By.NAME, "dadosConsulta.cdCaderno")
           )
           caderno_select.select_by_value(settings.target_caderno)  # "12"
           
           await asyncio.sleep(1)
           
           # 3. Configurar palavras-chave ESPEC√çFICAS
           palavras_input = self.driver.find_element(By.NAME, "dadosConsulta.pesquisaLivre")
           palavras_input.clear()
           palavras_input.send_keys(settings.search_terms)  # "RPV" E "pagamento pelo INSS"
           
           logger.info(
               "Search parameters configured successfully",
               date=date_str,
               caderno=settings.target_caderno,
               search_query=settings.search_terms
           )
           
           return True
           
       except Exception as e:
           logger.error("Failed to configure search parameters", error=str(e))
           return False
   
   async def execute_search(self) -> bool:
       """üîç Executar busca"""
       try:
           logger.info("Executing search")
           
           # Encontrar e clicar no bot√£o "Pesquisar"
           search_button = self.wait.until(
               EC.element_to_be_clickable((By.XPATH, "//input[@type='submit'][@value='Pesquisar']"))
           )
           
           search_button.click()
           
           # Aguardar resultados carregarem
           try:
               # Wait for results container or error message
               self.wait.until(
                   EC.any_of(
                       EC.presence_of_element_located((By.ID, "divResultadosInferior")),
                       EC.presence_of_element_located((By.CLASS_NAME, "erro")),
                       EC.text_to_be_present_in_element((By.TAG_NAME, "body"), "Nenhum resultado")
                   )
               )
               
               logger.info("Search executed successfully")
               return True
               
           except TimeoutException:
               logger.warning("Search results took too long to load")
               return False
               
       except Exception as e:
           logger.error("Failed to execute search", error=str(e))
           return False
   
   async def extract_publications_from_results(self) -> List[PublicationData]:
       """üìÑ Extrair publica√ß√µes dos PDFs individuais - COM DEBUG MELHORADO"""
       publications = []
       
       try:
           # 1. Obter links dos PDFs da p√°gina de resultados
           pdf_links = await self._extract_pdf_links_from_search_results()
           
           logger.info(f"Found {len(pdf_links)} PDF links to process")
           
           # 2. Processar cada PDF individualmente
           for i, pdf_url in enumerate(pdf_links, 1):
               try:
                   logger.info(f"Processing PDF {i}/{len(pdf_links)}: {pdf_url}")
                   
                   # Baixar PDF
                   pdf_content = await self._download_pdf(pdf_url)
                   if not pdf_content:
                       continue
                   
                   # Extrair texto do PDF
                   pdf_text = await self._extract_text_from_pdf(pdf_content)
                   if not pdf_text:
                       continue
                   
                                    
                   # REMOVER valida√ß√£o no n√≠vel do PDF - processar sempre
                   logger.info(f"PDF {i}: Processando se√ß√µes independente da valida√ß√£o geral")
                   
                   # Extrair publica√ß√µes do texto (pode ter m√∫ltiplas)
                   page_publications = await self._extract_publications_from_text(pdf_text, pdf_url)
                   
                   for pub in page_publications:
                       if pub:
                           publications.append(pub)
                           logger.info(f"Publica√ß√£o extra√≠da: {pub.process_number}")
                   
                   # Rate limiting
                   await asyncio.sleep(settings.dje_delay_between_requests)
                   
               except Exception as e:
                   logger.warning(f"Erro ao processar PDF {i}: {e}")
                   continue
           
           logger.info(f"Total de publica√ß√µes v√°lidas extra√≠das: {len(publications)}")
           
       except Exception as e:
           logger.error(f"Erro na extra√ß√£o geral: {e}")
       
       return publications
   
   async def _debug_extracted_text(self, text: str, pdf_index: int):
       """üöÄ DEBUG: Log detalhado do texto extra√≠do"""
       try:
           text_preview = text[:1000] if text else "TEXTO VAZIO"
           text_length = len(text) if text else 0
           
           logger.info(
               f"üîç DEBUG PDF {pdf_index} - Texto extra√≠do",
               text_length=text_length,
               text_preview=text_preview,
               has_rpv_term=bool(re.search(r'\bRPV\b', text, re.IGNORECASE)) if text else False,
               has_inss_payment=bool(re.search(r'pagamento pelo INSS', text, re.IGNORECASE)) if text else False,
               encoding_info=type(text).__name__
           )
           
           # Salvar texto completo em arquivo para debug (opcional)
           if settings.debug_save_extracted_text:
               with open(f"debug_pdf_{pdf_index}_text.txt", "w", encoding="utf-8") as f:
                   f.write(text)
               logger.debug(f"Texto completo salvo em debug_pdf_{pdf_index}_text.txt")
               
       except Exception as e:
           logger.error(f"Erro no debug do texto extra√≠do: {e}")
   
   async def _extract_pdf_links_from_search_results(self) -> List[str]:
       """üîó Extrair URLs dos PDFs da p√°gina de resultados"""
       import html
       from urllib.parse import urljoin, urlparse, urlunparse
       
       links = []

       try:
           # Verificar se h√° resultados
           try:
               self.driver.find_element(By.ID, "divResultadosInferior")
           except NoSuchElementException:
               logger.info("Nenhum container de resultados encontrado")
               return []

           page_source = self.driver.page_source
           soup = BeautifulSoup(page_source, 'html.parser')

           # Procurar links com popup() - padr√£o do DJE
           popup_links = soup.find_all('a', onclick=re.compile(r"popup\('(/cdje/consultaSimples\.do\?[^']+)'\)"))

           for link in popup_links:
               onclick = link.get('onclick', '')
               match = re.search(r"popup\('(/cdje/consultaSimples\.do\?[^']+)'\)", onclick)
               if match:
                   relative_url = match.group(1)
                   # Normalizar URL diretamente aqui
                   unescaped = html.unescape(relative_url)
                   parsed = urlparse(unescaped)
                   normalized_url = urlunparse(parsed)
                   full_url = urljoin(self.base_url, normalized_url)
                   links.append(full_url)

           # Remover duplicatas mantendo ordem
           unique_links = []
           for link in links:
               if link not in unique_links:
                   unique_links.append(link)

           logger.info(f"Extracted {len(unique_links)} unique PDF links")
           return unique_links

       except Exception as e:
           logger.error(f"Erro ao extrair links dos PDFs: {e}")
           return []
  
   async def _download_pdf(self, pdf_url: str) -> Optional[bytes]:
       """üì• Baixar PDF individual"""
       try:
           # Construir URL direta do PDF baseada na URL de consulta
           if "consultaSimples.do" in pdf_url:
               # Extrair par√¢metros da URL de consulta
               from urllib.parse import urlparse, parse_qs
               parsed = urlparse(pdf_url)
               params = parse_qs(parsed.query)
               
               # Construir URL direta do PDF
               pdf_direct_url = f"https://dje.tjsp.jus.br/cdje/getPaginaDoDiario.do?cdVolume={params['cdVolume'][0]}&nuDiario={params['nuDiario'][0]}&cdCaderno={params['cdCaderno'][0]}&nuSeqpagina={params['nuSeqpagina'][0]}&uuidCaptcha="
               
               logger.debug(f"Converted to direct PDF URL: {pdf_direct_url}")
               pdf_url = pdf_direct_url
           
           response = await self.http_client.get(pdf_url)
           response.raise_for_status()
           
           # Verificar tamanho do arquivo
           content_length = len(response.content)
           max_size = settings.pdf_max_size_mb * 1024 * 1024
           
           if content_length > max_size:
               logger.warning(f"PDF muito grande ({content_length / 1024 / 1024:.1f}MB), pulando")
               return None
           
           # Verificar se √© PDF
           content_type = response.headers.get('content-type', '').lower()
           if 'pdf' in content_type or content_length > 1000:  # PDFs s√£o geralmente maiores que 1KB
               return response.content
           
           logger.warning(f"Response n√£o √© PDF: content-type={content_type}, size={content_length}")
           return None
           
       except Exception as e:
           logger.error(f"Erro ao baixar PDF {pdf_url}: {e}")
           return None
  
   async def _extract_text_from_pdf(self, pdf_content: bytes) -> Optional[str]:
       """üìÑ Extrair texto do PDF com fallback OCR - COM DEBUG"""
       try:
           # Tentar extrair como PDF texto primeiro
           try:
               with pdfplumber.open(io.BytesIO(pdf_content)) as pdf:
                   text = ""
                   for page_num, page in enumerate(pdf.pages):
                       page_text = page.extract_text()
                       if page_text:
                           text += page_text + "\n"
                   
                   if text and len(text.strip()) > 100:
                       return text
                       
           except Exception as e:
               logger.debug(f"PDF text extraction failed: {e}, trying OCR")
           
           # Fallback: OCR se o texto for insuficiente
           try:
               logger.debug("Attempting OCR extraction...")
               with pdfplumber.open(io.BytesIO(pdf_content)) as pdf:
                   text = ""
                   for page_num, page in enumerate(pdf.pages):
                       try:
                           # Converter p√°gina em imagem
                           img = page.to_image(resolution=300)
                           
                           # Aplicar OCR
                           page_text = pytesseract.image_to_string(
                               img.original, 
                               lang=settings.ocr_language,
                               config=settings.ocr_config
                           )
                           text += page_text + "\n"
                           logger.debug(f"OCR page {page_num}: {len(page_text)} chars")
                           
                       except Exception as e:
                           logger.warning(f"OCR failed for page {page_num}: {e}")
                           continue
                   
                   logger.debug(f"üîç OCR extraction: {len(text)} total chars extracted")
                   
                   if text and len(text.strip()) > 50:
                       logger.debug("OCR text extracted successfully")
                       return text
                   else:
                       logger.warning(f"OCR text too short: {len(text.strip())} chars")
                       
           except Exception as e:
               logger.warning(f"OCR extraction failed: {e}")
           
           return None
           
       except Exception as e:
           logger.error(f"Erro na extra√ß√£o de texto do PDF: {e}")
           return None
  
   def _validate_required_keywords(self, text: str) -> bool:
       """‚úÖ Validar palavras-chave obrigat√≥rias: RPV + pagamento pelo INSS - COM DEBUG"""
       try:
           if not text:
               return False
               
           has_rpv = bool(re.search(r'\bRPV\b', text, re.IGNORECASE))
           has_inss_payment = bool(re.search(r'pagamento pelo INSS', text, re.IGNORECASE))
           
           return has_rpv and has_inss_payment
           
       except Exception as e:
           logger.error(f"Erro na valida√ß√£o de palavras-chave: {e}")
           return False
  
   async def _extract_publications_from_text(self, text: str, source_url: str) -> List[PublicationData]:
        """üìã Extrair m√∫ltiplas publica√ß√µes do texto"""
        publications = []
        
        # ‚úÖ EXTRAIR DATA UMA VEZ DO CABE√áALHO COMPLETO
        publication_date = self._extract_publication_date_from_header(text)
        
        try:
            # Separar por processos individuais
            process_sections = re.split(r'(?=Processo \d{7}-\d{2}\.\d{4}\.\d\.\d{2}\.\d{4})', text)
            
            for i, section in enumerate(process_sections):
                if not section.strip():
                    continue
                
                if not self._validate_required_keywords(section):
                    continue
            
                publication = await self._extract_single_publication_from_text(
                    section, source_url, publication_date
                )
                if publication:
                    publications.append(publication)
            
            return publications
        except Exception as e:
            logger.error(f"Erro ao extrair publica√ß√µes do texto: {e}")
            return []
  
   def _extract_publication_date_from_header(self, full_text: str) -> Optional[date]:
        """üìÖ Extrair data de disponibiliza√ß√£o do cabe√ßalho do DJE"""
        months = {
            'janeiro': 1, 'fevereiro': 2, 'mar√ßo': 3, 'abril': 4,
            'maio': 5, 'junho': 6, 'julho': 7, 'agosto': 8,
            'setembro': 9, 'outubro': 10, 'novembro': 11, 'dezembro': 12
        }
         
        pattern = r'Disponibiliza√ß√£o:\s*([^,]+),\s*(\d{1,2})\s+de\s+([a-z√°√™√ß√µ√£√†√©√≠√≥√∫√¢√Æ√¥√ª]+)\s+de\s+(\d{4})'
        match = re.search(pattern, full_text, re.IGNORECASE)
        
        if match:
            try:
                day_of_week = match.group(1).strip()
                day = int(match.group(2))
                month_name = match.group(3)
                year = int(match.group(4))
                
                logger.debug(f"üóìÔ∏è Dados extra√≠dos: dia_semana='{day_of_week}', dia={day}, m√™s='{month_name}', ano={year}")
                
                if month_name in months:
                    publication_date = date(year, months[month_name], day)
                    logger.debug(f"‚úÖ Data extra√≠da do cabe√ßalho: {publication_date}")
                    return publication_date
                else:
                    logger.warning(f"‚ùå M√™s '{month_name}' n√£o encontrado no dicion√°rio")
                    
            except (ValueError, KeyError) as e:
                logger.warning(f"Erro ao converter data: {e}")
        
        logger.debug("‚ùå N√£o foi poss√≠vel extrair data do cabe√ßalho")
        return None


   def extract_authors_improved(text):
        """Extrair autores com padr√µes melhorados para DJE"""
        authors = []
        
        # üéØ PADR√ïES ESPEC√çFICOS PARA DJE (mais eficazes primeiro)
        author_patterns = [
            
            # 1. PADR√ÉO PRINCIPAL - DIREITO PREVIDENCI√ÅRIO
            # "- DIREITO PREVIDENCI√ÅRIO - [NOME] - Vistos"
            r'-\s*DIREITO PREVIDENCI√ÅRIO\s*-\s*([A-Z√Å√ä√á√ï√É√Ä√â√ç√ì√ö√Ç√é√î√õ][a-z√°√™√ß√µ√£√†√©√≠√≥√∫√¢√Æ√¥√ª√ß\s]+?)\s*-\s*Vistos',
            
            # 2. PADR√ÉO PARA DIFERENTES TIPOS DE BENEF√çCIO
            # "- [TIPO BENEF√çCIO] - [NOME] - Vistos"
            r'-\s*(?:Aux√≠lio-Acidente|Aux√≠lio-Doen√ßa|Aposentadoria|Benef√≠cios em Esp√©cie|Incapacidade Laborativa)\s*(?:\([^)]+\))?\s*-\s*([A-Z√Å√ä√á√ï√É√Ä√â√ç√ì√ö√Ç√é√î√õ][a-z√°√™√ß√µ√£√†√©√≠√≥√∫√¢√Æ√¥√ª√ß\s]+?)\s*-\s*Vistos',
            
            # 3. PADR√ÉO GEN√âRICO PARA CUMPRIMENTO DE SENTEN√áA
            # "- Cumprimento [qualquer coisa] - [NOME] - Vistos"
            r'-\s*Cumprimento\s+[^-]+\s*-\s*([A-Z√Å√ä√á√ï√É√Ä√â√ç√ì√ö√Ç√é√î√õ][a-z√°√™√ß√µ√£√†√©√≠√≥√∫√¢√Æ√¥√ª√ß\s]+?)\s*-\s*Vistos',
            
            # 4. PADR√ÉO MAIS GEN√âRICO - qualquer coisa antes do nome
            # "- [qualquer texto] - [NOME] - Vistos"
            r'-\s*[^-]+\s*-\s*([A-Z√Å√ä√á√ï√É√Ä√â√ç√ì√ö√Ç√é√î√õ][a-z√°√™√ß√µ√£√†√©√≠√≥√∫√¢√Æ√¥√ª√ß\s]{8,50}?)\s*-\s*Vistos',
            
            # 5. PADR√ÉO DE FALLBACK - nome diretamente antes de "Vistos"
            # "[NOME] - Vistos"
            r'([A-Z√Å√ä√á√ï√É√Ä√â√ç√ì√ö√Ç√é√î√õ][a-z√°√™√ß√µ√£√†√©√≠√≥√∫√¢√Æ√¥√ª√ß\s]{8,50}?)\s*-\s*Vistos',
            
            # 6. PADR√ÉO ALTERNATIVO - buscar ap√≥s processo principal
            # "(processo principal [n√∫mero]) - [tipo] - [NOME] - Vistos"
            r'\(processo principal [^)]+\)\s*-[^-]*-\s*([A-Z√Å√ä√á√ï√É√Ä√â√ç√ì√ö√Ç√é√î√õ][a-z√°√™√ß√µ√£√†√©√≠√≥√∫√¢√Æ√¥√ª√ß\s]+?)\s*-\s*Vistos'
        ]
        
        for i, pattern in enumerate(author_patterns):
            print(f"üîç Testando padr√£o {i+1}: {pattern}")
            
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            print(f"   Matches encontrados: {matches}")
            
            for match in matches:
                if isinstance(match, tuple):
                    author_name = match[-1].strip()
                else:
                    author_name = match.strip()
                
                # Validar nome
                if validate_author_name(author_name):
                    author_name = clean_author_name(author_name)
                    if author_name not in authors:
                        authors.append(author_name)
                        print(f"‚úÖ Autor extra√≠do: {author_name}")
                        return authors  # Retorna assim que encontra o primeiro
        
        print("‚ùå Nenhum autor encontrado")
        return authors

   def validate_author_name(name):
        """Validar se o nome extra√≠do √© v√°lido"""
        if not name or len(name.strip()) < 5:
            return False
        
        # Rejeitar se cont√©m apenas palavras comuns de documentos jur√≠dicos
        invalid_words = [
            'vistos', 'tendo', 'processo', 'cumprimento', 'senten√ßa', 
            'direito', 'previdenci√°rio', 'art', 'homologa√ß√£o'
        ]
        
        name_lower = name.lower()
        if any(word in name_lower for word in invalid_words):
            return False
        
        # Deve ter pelo menos 2 palavras (nome e sobrenome)
        words = name.split()
        if len(words) < 2:
            return False
        
        return True

   def clean_author_name(name):
        """Limpar e formatar nome do autor"""
        # Remover caracteres especiais no in√≠cio/fim
        name = re.sub(r'^[^\w\s]+|[^\w\s]+$', '', name)
        
        # Normalizar espa√ßos
        name = re.sub(r'\s+', ' ', name.strip())
        
        # Capitalizar cada palavra
        return ' '.join(word.capitalize() for word in name.split())


    
   async def _extract_single_publication_from_text(self, text: str, source_url: str, publication_date: Optional[date] = None) -> Optional[PublicationData]:
        """üìã Extrair dados de uma √∫nica publica√ß√£o do texto - VERS√ÉO CORRIGIDA AUTORES"""
        try:
            # Extrair n√∫mero do processo
            process_match = re.search(r'Processo (\d{7}-\d{2}\.\d{4}\.\d\.\d{2}\.\d{4})', text)
            if not process_match:
                logger.debug("‚ùå N√∫mero do processo n√£o encontrado")
                return None
            
            process_number = process_match.group(1)
            logger.debug(f"‚úÖ Processo encontrado: {process_number}")
            
            # ‚úÖ EXTRA√á√ÉO DE AUTORES CORRIGIDA - PADR√ÉO REAL DO DJE
            authors = []
            
            # PADR√ÉO PRINCIPAL: - [NOME] - Vistos
            # Baseado nos exemplos reais: "- Josuel Anderson de Oliveira - Vistos"
            author_pattern = r'-\s*([A-Z√Å√ä√á√ï√É√Ä√â√ç√ì√ö√Ç√é√î√õ][a-z√°√™√ß√µ√£√†√©√≠√≥√∫√¢√Æ√¥√ª√ß]+(?:\s+[A-Z√Å√ä√á√ï√É√Ä√â√ç√ì√ö√Ç√é√î√õ][a-z√°√™√ß√µ√£√†√©√≠√≥√∫√¢√Æ√¥√ª√ß]+)+)\s*-\s*Vistos\.?'
            
            matches = re.findall(author_pattern, text, re.IGNORECASE | re.MULTILINE)
            logger.debug(f"üîç Testando padr√£o principal de autores: {author_pattern}")
            logger.debug(f"üîç Matches encontrados: {matches}")
            
            for match in matches:
                author_name = match.strip()
                # Valida√ß√£o b√°sica do nome
                if len(author_name) >= 5 and len(author_name.split()) >= 2:
                    # Limpar e formatar nome
                    author_name = re.sub(r'\s+', ' ', author_name.strip())
                    author_name = ' '.join(word.capitalize() for word in author_name.split())
                    
                    if author_name not in authors:
                        authors.append(author_name)
                        logger.debug(f"‚úÖ Autor extra√≠do: {author_name}")
            
            # FALLBACK: Se n√£o encontrou com o padr√£o principal, tentar outros padr√µes
            if not authors:
                logger.debug("‚ùå Nenhum autor encontrado com padr√£o principal, tentando fallback...")
                
                # Padr√£o fallback mais amplo
                fallback_patterns = [
                    # 1. Casos como "DIREITO PREVIDENCI√ÅRIO - Nome - Vistos"
                    r'DIREITO PREVIDENCI√ÅRIO\s*-\s*([A-Z√Å√Ç√É√â√ä√ç√ì√î√ï√ö√á][a-z√°√¢√£√©√™√≠√≥√¥√µ√∫√ß]+(?:\s+[A-Z√Å√Ç√É√â√ä√ç√ì√î√ï√ö√á][a-z√°√¢√£√©√™√≠√≥√¥√µ√∫√ß]+)+)\s*-\s*Vistos[.:]?',
                    
                    # 2. Casos com tipo de benef√≠cio antes do nome
                    r'(?:Aux√≠lio-Acidente|Aux√≠lio-Doen√ßa|Aposentadoria|Benef√≠cios em Esp√©cie|Incapacidade Laborativa)[^-]*-\s*([A-Z√Å√Ç√É√â√ä√ç√ì√î√ï√ö√á][a-z√°√¢√£√©√™√≠√≥√¥√µ√∫√ß]+(?:\s+[A-Z√Å√Ç√É√â√ä√ç√ì√î√ï√ö√á][a-z√°√¢√£√©√™√≠√≥√¥√µ√∫√ß]+)+)\s*-\s*Vistos[.:]?',
                    
                    # 3. Padr√£o gen√©rico: nome composto antes de "- Vistos"
                    r'-\s*([A-Z√Å√Ç√É√â√ä√ç√ì√î√ï√ö√á][a-z√°√¢√£√©√™√≠√≥√¥√µ√∫√ß]+(?:\s+[A-Z√Å√Ç√É√â√ä√ç√ì√î√ï√ö√á][a-z√°√¢√£√©√™√≠√≥√¥√µ√∫√ß]+)+)\s*-\s*Vistos[.:]?'
                ]
                                
                for i, pattern in enumerate(fallback_patterns):
                    matches = re.findall(pattern, text, re.IGNORECASE)
                    logger.debug(f"üîç Fallback pattern {i+1}: {len(matches)} matches")
                    
                    for match in matches:
                        author_name = match.strip()
                        if len(author_name) >= 5 and len(author_name.split()) >= 2:
                            author_name = re.sub(r'\s+', ' ', author_name.strip())
                            author_name = ' '.join(word.capitalize() for word in author_name.split())
                            
                            if author_name not in authors:
                                authors.append(author_name)
                                logger.debug(f"‚úÖ Autor extra√≠do com fallback {i+1}: {author_name}")
                                break
                    
                    if authors:  # Se encontrou, para aqui
                        break
            
            if not authors:
                logger.debug("‚ùå Nenhum autor encontrado mesmo com fallback")
            
            # Extrair advogado (mantendo o c√≥digo existente)
            lawyers = []
            lawyer_patterns = [
                r'ADV: ([A-Z√Å√ä√á√ï\s]+?) \(OAB (\d+\/SP)\)',
                r'Int\. - ADV: ([A-Z√Å√ä√á√ï\s]+?) \(OAB (\d+\/SP)\)'
            ]
            
            for pattern in lawyer_patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    lawyer_name = match[0].strip()
                    oab_number = match[1]
                    lawyers.append(f"{lawyer_name} (OAB {oab_number})")
                    logger.debug(f"‚úÖ Advogado encontrado: {lawyer_name} (OAB {oab_number})")
            
            # Extrair valores monet√°rios (mantendo o c√≥digo que j√° est√° funcionando)
            main_value = None
            interest_value = None
            legal_fees = None
            
            # Valor principal
            main_patterns = [
                r'R\$ ([\d.,]+) - principal\s*bruto\/?\s*l√≠quido?',
                r'R\$ ([\d.,]+) - principal',
                r'valor.*?principal.*?R\$ ([\d.,]+)',
                r'importe total de R\$ ([\d.,]+)'
            ]
            
            for pattern in main_patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    main_value = self._parse_monetary_value(match.group(1))
                    logger.debug(f"‚úÖ Valor principal: {main_value}")
                    break
            
            # Juros morat√≥rios
            if re.search(r'sem juros morat√≥rios', text, re.IGNORECASE):
                interest_value = Decimal('0.00')
                logger.debug("‚úÖ Sem juros morat√≥rios")
            else:
                interest_patterns = [
                    r'R\$ ([\d.,]+) - juros morat√≥rios',
                    r'juros.*?R\$ ([\d.,]+)',
                    r'corre√ß√£o.*?R\$ ([\d.,]+)'
                ]
                
                for pattern in interest_patterns:
                    match = re.search(pattern, text, re.IGNORECASE)
                    if match:
                        interest_value = self._parse_monetary_value(match.group(1))
                        logger.debug(f"‚úÖ Juros morat√≥rios: {interest_value}")
                        break
            
            # Honor√°rios advocat√≠cios
            fees_patterns = [
                r'R\$ ([\d.,]+) - honor√°rios advocat√≠cios',
                r'honor√°rios.*?R\$ ([\d.,]+)',
                r'verba.*?honor√°ria.*?R\$ ([\d.,]+)'
            ]
            
            for pattern in fees_patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    legal_fees = self._parse_monetary_value(match.group(1))
                    logger.debug(f"‚úÖ Honor√°rios: {legal_fees}")
                    break
            
            publication = PublicationData(
                process_number=process_number,
                authors=authors,
                lawyers=lawyers,
                full_content=text,
                source_url=source_url,
                publication_date=publication_date,
                availability_date=publication_date,
                main_value=main_value,
                interest_value=interest_value,
                legal_fees=legal_fees,
                scraper_execution_id=self.current_execution_id
            )
            
            logger.debug(f"üìä Publica√ß√£o criada: {process_number}, autores: {len(authors)} ({authors}), v√°lida: {publication.is_valid()}")
            return publication
            
        except Exception as e:
            logger.error(f"Erro ao extrair dados da publica√ß√£o: {e}")
            return None
  
   def _parse_monetary_value(self, value_str: str) -> Optional[Decimal]:
       """üí∞ Converter string monet√°ria brasileira para Decimal - COM DEBUG"""
       if not value_str or value_str.strip() in ['.', ',', '-', '']:
           logger.debug(f"üöÄ DEBUG: Valor monet√°rio inv√°lido/vazio: '{value_str}'")
           return None
       
       try:
           # Limpar string
           clean_value = value_str.strip()
           logger.debug(f"üöÄ DEBUG: Parsing valor: '{value_str}' -> '{clean_value}'")
           
          # Formato brasileiro: 1.234,56
           if ',' in clean_value and clean_value.count('.') > 0:
               # Remove separadores de milhar e converte v√≠rgula para ponto
               clean_value = clean_value.replace('.', '').replace(',', '.')
           elif ',' in clean_value:
               # Apenas v√≠rgula, converter para ponto
               clean_value = clean_value.replace(',', '.')
           
           # Verificar se sobrou algo v√°lido
           if not clean_value or clean_value in ['.', ',']:
               logger.debug(f"üöÄ DEBUG: Valor limpo inv√°lido: '{clean_value}'")
               return None
           
           result = Decimal(clean_value)
           logger.debug(f"üöÄ DEBUG: Convers√£o bem-sucedida: '{value_str}' -> {result}")
           return result
               
       except (ValueError, AttributeError, Exception) as e:
           logger.debug(f"üöÄ DEBUG: Erro ao converter valor '{value_str}': {e}")
           return None
   
   async def navigate_to_next_page(self) -> bool:
      """‚û°Ô∏è Navegar para pr√≥xima p√°gina usando JavaScript"""
      try:
          # Procurar link "Pr√≥ximo>"
          next_links = self.driver.find_elements(
              By.XPATH, 
              "//a[contains(text(), 'Pr√≥ximo>') or contains(text(), 'Pr√≥ximo')]"
          )
          
          for link in next_links:
              if link.is_enabled() and link.is_displayed():
                  onclick = link.get_attribute('onclick')
                  if onclick and 'trocaDePg' in onclick:
                      # Extrair n√∫mero da p√°gina
                      page_match = re.search(r'trocaDePg\((\d+)\)', onclick)
                      if page_match:
                          next_page = page_match.group(1)
                          logger.debug(f"Navigating to page {next_page}")
                          
                          # Executar JavaScript diretamente
                          self.driver.execute_script(f"trocaDePg({next_page});")
                          
                          # Aguardar nova p√°gina carregar
                          await asyncio.sleep(3)
                          
                          # Verificar se mudou de p√°gina
                          try:
                              self.wait.until(
                                  EC.presence_of_element_located((By.ID, "divResultadosInferior"))
                              )
                              logger.info(f"Successfully navigated to page {next_page}")
                              return True
                          except TimeoutException:
                              logger.warning("Next page did not load properly")
                              return False
          
          logger.info("No next page link found or enabled")
          return False
          
      except Exception as e:
          logger.warning("Failed to navigate to next page", error=str(e))
          return False
  
   async def scrape_publications(
      self, 
      target_date: date,
      execution_id: int
  ) -> ScrapingResult:
      """üï∑Ô∏è M√©todo principal de scraping com DEBUG MELHORADO"""
      
      self.current_execution_id = execution_id
      result = ScrapingResult()
      start_time = time.time()
      
      try:
          logger.info(
              "Starting DJE scraping (PDF STRATEGY - DEBUG MODE)",
              target_date=target_date.isoformat(),
              execution_id=execution_id
          )
          
          # Setup driver se necess√°rio
          if not self.driver:
              await self.setup_driver()
          
          # 1. Navegar para p√°gina de busca
          if not await self.navigate_to_search_page():
              raise DJEScraperError("Failed to navigate to search page")
          
          # 2. Configurar par√¢metros de busca
          if not await self.configure_search_parameters(target_date):
              raise DJEScraperError("Failed to configure search parameters")
          
          # 3. Executar busca
          if not await self.execute_search():
              raise DJEScraperError("Failed to execute search")
          
          # 4. Processar resultados p√°gina por p√°gina
          current_page = 1
          max_pages = settings.max_pages_per_execution
          
          while current_page <= max_pages:
              logger.info(f"Processing page {current_page}")
              
              # Extrair publica√ß√µes da p√°gina atual (estrat√©gia PDF com DEBUG)
              page_publications = await self.extract_publications_from_results()
              
              result.total_found += len(page_publications)
              result.pages_scraped = current_page
              
              # Adicionar publica√ß√µes v√°lidas ao resultado
              valid_count = 0
              for publication in page_publications:
                  if publication and publication.is_valid():
                      result.add_publication(publication)
                      valid_count += 1
                      logger.info(f"‚úÖ Publica√ß√£o v√°lida adicionada: {publication.process_number}")
                  else:
                      if publication:
                          result.add_error(f"Publica√ß√£o inv√°lida: {publication.process_number}")
                          logger.warning(f"‚ùå Publica√ß√£o inv√°lida: {publication.process_number}")
                      else:
                          result.add_error("Falha na extra√ß√£o de publica√ß√£o")
              
              logger.info(
                  f"üìä Page {current_page} processed",
                  publications_found=len(page_publications),
                  valid_publications=valid_count
              )
              
              # Tentar ir para pr√≥xima p√°gina
              if current_page < max_pages:
                  if await self.navigate_to_next_page():
                      current_page += 1
                      # Delay entre p√°ginas
                      await asyncio.sleep(settings.dje_delay_between_requests)
                  else:
                      logger.info("No more pages available")
                      break
              else:
                  logger.info(f"Reached maximum pages limit ({max_pages})")
                  break
          
          result.execution_time = time.time() - start_time
          
          logger.info(
              "DJE scraping completed successfully",
              execution_id=execution_id,
              **result.get_summary()
          )
          
      except Exception as e:
          result.add_error(f"Scraping failed: {str(e)}")
          result.execution_time = time.time() - start_time
          
          logger.error(
              "DJE scraping failed",
              execution_id=execution_id,
              error=str(e),
              execution_time=result.execution_time
          )
          
          raise
      
      return result
  
   async def close(self):
      """üîí Fechar driver e recursos"""
      if self.http_client:
          await self.http_client.aclose()
      
      if self.driver:
          try:
              self.driver.quit()
              logger.info("Chrome driver closed successfully")
          except Exception as e:
              logger.warning("Error closing driver", error=str(e))
          finally:
              self.driver = None
              self.wait = None

   def normalize_link(url: str) -> str:
      """üîó Normalizar URL removendo entidades HTML"""
      import html
      from urllib.parse import urlparse, urlunparse
   
      # Desescapa entidades HTML como &amp;
      unescaped = html.unescape(url)
      parsed = urlparse(unescaped)
      # Normaliza a URL (sem ordenar query, s√≥ padroniza)
      return urlunparse(parsed)

# Singleton instance
_scraper: Optional[DJEScraper] = None

async def get_dje_scraper() -> DJEScraper:
  """üï∑Ô∏è Get singleton DJE scraper instance"""
  global _scraper
  
  if _scraper is None:
      _scraper = DJEScraper()
  
  return _scraper

async def close_dje_scraper():
  """üîí Close singleton DJE scraper"""
  global _scraper
  
  if _scraper:
      await _scraper.close()
      _scraper = None